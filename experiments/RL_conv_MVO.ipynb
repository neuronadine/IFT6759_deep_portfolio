{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from statsmodels.tsa.regime_switching.markov_regression import MarkovRegression\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "folder_path = os.path.join(\"stocks\", \"datasets\", \"dj30\", \"raw\", \"*.csv\")\n",
    "csv_files = glob.glob(folder_path)\n",
    "data_frames = []\n",
    "\n",
    "for file in csv_files:\n",
    "    symbol = os.path.splitext(os.path.basename(file))[0]\n",
    "    print(f\"Processing {symbol} from {file}...\")\n",
    "    df = pd.read_csv(file, parse_dates=['Date'])\n",
    "    # Use data from 2010 to 2019\n",
    "    mask = (df['Date'] >= '2010-01-01') & (df['Date'] <= '2019-12-31')\n",
    "    df = df.loc[mask]\n",
    "    df = df[['Date', 'Adj Close']].set_index('Date')\n",
    "    df.rename(columns={'Adj Close': symbol}, inplace=True)\n",
    "    data_frames.append(df)\n",
    "\n",
    "merged_df = pd.concat(data_frames, axis=1)\n",
    "merged_df.sort_index(inplace=True)\n",
    "train_data = merged_df.loc['2010-01-01':'2016-01-01']\n",
    "test_data = merged_df.loc['2016-01-01':'2018-01-01']\n",
    "\n",
    "\n",
    "\n",
    "class PortfolioEnv:\n",
    "    def __init__(self, price_data, window_obs=60, window_state=120):\n",
    "        self.price_data = price_data.reset_index(drop=True)\n",
    "        self.window_obs = window_obs\n",
    "        self.window_state = window_state\n",
    "        self.n_assets = price_data.shape[1]\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = max(self.window_obs, self.window_state)\n",
    "        self.done = False\n",
    "        self.portfolio_value = 1.0\n",
    "        self.history = [self.portfolio_value]\n",
    "        self.weights = np.ones(self.n_assets) / self.n_assets\n",
    "        return self._get_state()\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.weights = action\n",
    "        current_prices = self.price_data.iloc[self.current_step].values\n",
    "        next_prices = self.price_data.iloc[self.current_step+1].values\n",
    "        asset_returns = (next_prices / current_prices) - 1\n",
    "        portfolio_return = np.dot(self.weights, asset_returns)\n",
    "        self.portfolio_value *= (1 + portfolio_return)\n",
    "        self.history.append(self.portfolio_value)\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.price_data)-1:\n",
    "            self.done = True\n",
    "        reward = portfolio_return\n",
    "        return self._get_state(), reward, self.done, {}\n",
    "    \n",
    "    def _get_state(self):\n",
    "        obs_data = self.price_data.iloc[self.current_step - self.window_obs:self.current_step]\n",
    "        obs_returns = np.log(obs_data / obs_data.shift(1)).dropna().values.T\n",
    "        if obs_returns.shape[1] < self.window_obs:\n",
    "            pad = np.zeros((self.n_assets, self.window_obs - obs_returns.shape[1]))\n",
    "            obs_returns = np.hstack((obs_returns, pad))\n",
    "    \n",
    "        hist = np.array(self.history)\n",
    "        if len(hist) < self.window_state + 1:\n",
    "            state_data = np.log(hist[1:] / hist[:-1])\n",
    "            state_data = np.pad(state_data, (self.window_state - len(state_data), 0), 'constant')\n",
    "        else:\n",
    "            window_hist = hist[-(self.window_state+1):]\n",
    "            state_data = np.log(window_hist[1:] / window_hist[:-1])\n",
    "        state_data = state_data[np.newaxis, :] \n",
    "        \n",
    "        obs_tensor = torch.tensor(obs_returns, dtype=torch.float32).unsqueeze(0)  \n",
    "        state_tensor = torch.tensor(state_data, dtype=torch.float32).unsqueeze(0) \n",
    "        return (obs_tensor, state_tensor)\n",
    "    \n",
    "    def render(self):\n",
    "        plt.plot(self.history)\n",
    "        plt.title(\"Portfolio Value\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def generate_actions(n_assets, n_actions=50):\n",
    "    actions = []\n",
    "    for _ in range(n_actions):\n",
    "        w = np.random.rand(n_assets)\n",
    "        w = w / w.sum()\n",
    "        actions.append(w)\n",
    "    return np.array(actions)\n",
    "\n",
    "\n",
    "n_assets = train_data.shape[1]\n",
    "actions = generate_actions(n_assets, n_actions=50)\n",
    "\n",
    "\n",
    "class RLPortfolioManager(nn.Module):\n",
    "    def __init__(self, obs_channels, obs_length, state_channels, state_length, num_actions):\n",
    "        super(RLPortfolioManager, self).__init__()\n",
    "        self.obs_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=obs_channels, out_channels=32, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        conv_obs_length = obs_length - 4  \n",
    "        self.fc_obs = nn.Linear(32 * conv_obs_length, 128)\n",
    "        \n",
    "        self.state_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=state_channels, out_channels=16, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 16, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        conv_state_length = state_length - 4\n",
    "        self.fc_state = nn.Linear(16 * conv_state_length, 128)\n",
    "        \n",
    "        self.fc_combined = nn.Linear(128 + 128, 128)\n",
    "        self.policy_head = nn.Linear(128, num_actions)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, observation, state):\n",
    "        obs_features = self.obs_conv(observation)\n",
    "        obs_features = obs_features.view(obs_features.size(0), -1)\n",
    "        obs_features = F.relu(self.fc_obs(obs_features))\n",
    "        \n",
    "        state_features = self.state_conv(state)\n",
    "        state_features = state_features.view(state_features.size(0), -1)\n",
    "        state_features = F.relu(self.fc_state(state_features))\n",
    "        \n",
    "        combined = torch.cat([obs_features, state_features], dim=1)\n",
    "        combined = F.relu(self.fc_combined(combined))\n",
    "        policy_logits = self.policy_head(combined)\n",
    "        value = self.value_head(combined)\n",
    "        return policy_logits, value\n",
    "\n",
    "obs_channels = n_assets          \n",
    "obs_length = 60                   \n",
    "state_channels = 1                \n",
    "state_length = 120                \n",
    "num_actions = actions.shape[0]    \n",
    "\n",
    "policy_net = RLPortfolioManager(obs_channels, obs_length, state_channels, state_length, num_actions)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "def sample_episode_df(data, episode_length=252):\n",
    "    \"\"\"Randomly sample a contiguous episode (approximately one trading year) from historical data.\"\"\"\n",
    "    if len(data) <= episode_length:\n",
    "        return data.copy()\n",
    "    start_idx = np.random.randint(0, len(data) - episode_length)\n",
    "    episode_df = data.iloc[start_idx:start_idx + episode_length].copy()\n",
    "    return episode_df\n",
    "\n",
    "def train_agent(policy_net, optimizer, actions, train_data, num_episodes=100, window_obs=60, window_state=120, gamma=0.99):\n",
    "    for ep in range(num_episodes):\n",
    "        episode_df = sample_episode_df(train_data, episode_length=252)\n",
    "        env = PortfolioEnv(episode_df, window_obs=window_obs, window_state=window_state)\n",
    "        state = env.reset()\n",
    "        \n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            obs, port_state = state\n",
    "            policy_logits, value = policy_net(obs, port_state)\n",
    "            probs = F.softmax(policy_logits, dim=1)\n",
    "            m = torch.distributions.Categorical(probs)\n",
    "            action_idx = m.sample()\n",
    "            log_prob = m.log_prob(action_idx)\n",
    "            value = value.squeeze(1)\n",
    "            action = actions[action_idx.item()]\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "        \n",
    "        R = 0\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        \n",
    "        values = torch.stack(values).squeeze(1)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "        advantage = returns - values.detach()\n",
    "        policy_loss = -(log_probs * advantage).mean()\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        loss = policy_loss + value_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Episode {ep+1}/{num_episodes}, Loss: {loss.item():.4f}, Total Reward: {np.sum(rewards):.4f}\")\n",
    "    \n",
    "    return policy_net\n",
    "\n",
    "trained_policy = train_agent(policy_net, optimizer, actions, train_data, num_episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e970465",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = merged_df.loc['2016-01-01':'2018-01-01'] # As an example, we can loop over all test periods\n",
    "\n",
    "# MVO Implementation and Backtesting\n",
    "\n",
    "\n",
    "def calculate_mvo_weights(daily_returns_df, risk_free_rate_daily=0.0):\n",
    "    \"\"\"\n",
    "    Calculates weights for the maximum Sharpe ratio portfolio using historical daily returns.\n",
    "    Assumes daily_returns_df is a pandas DataFrame (window_size x n_assets).\n",
    "    \"\"\"\n",
    "    n_assets = daily_returns_df.shape[1]\n",
    "    asset_names = daily_returns_df.columns\n",
    "\n",
    "    # expected returns (mu) and covariance (sigma) from daily returns\n",
    "    # Use simple mean and covariance of the provided daily returns window\n",
    "    mu = daily_returns_df.mean()\n",
    "    Sigma = daily_returns_df.cov()\n",
    "\n",
    "    # objective function: negative Sharpe ratio (to minimize)\n",
    "    def neg_sharpe(weights):\n",
    "        # daily expected return and daily std dev\n",
    "        p_ret = np.dot(mu, weights)\n",
    "        p_var = np.dot(weights.T, np.dot(Sigma, weights))\n",
    "        p_vol = np.sqrt(p_var) if p_var > 0 else 0 # daily volatility\n",
    "\n",
    "        if p_vol == 0:\n",
    "             return -np.inf if p_ret > risk_free_rate_daily else 0\n",
    "\n",
    "        sharpe = (p_ret - risk_free_rate_daily) / p_vol\n",
    "        return -sharpe # Minimize negative Sharpe\n",
    "\n",
    "    # Constraints: Weights sum to 1 (fully invested) / Weights are non-negative\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    bounds = tuple((0, 1) for _ in range(n_assets)) \n",
    "\n",
    "    # initial guess: equal weights\n",
    "    init_guess = np.ones(n_assets) / n_assets\n",
    "\n",
    "    #optimization\n",
    "    result = minimize(neg_sharpe, init_guess, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "    if not result.success:\n",
    "        print(f\"Warning: MVO optimization failed ({result.message}). Falling back to equal weights.\")\n",
    "        return np.ones(n_assets) / n_assets\n",
    "\n",
    "    return result.x\n",
    "\n",
    "\n",
    "def run_mvo_backtest(price_data_full, test_period_data, lookback_window=252):\n",
    "    \"\"\"\n",
    "    Runs the daily rolling MVO backtest on the test_period_data.\n",
    "    Uses price_data_full for historical lookbacks.\n",
    "    \"\"\"\n",
    "    print(f\"\\nRunning MVO backtest with {lookback_window}-day rolling window...\")\n",
    "    \n",
    "    returns_full = price_data_full.pct_change()\n",
    "    test_dates = test_period_data.index\n",
    "    n_assets = test_period_data.shape[1]\n",
    "    asset_names = test_period_data.columns\n",
    "\n",
    "    mvo_weights_history = pd.DataFrame(index=test_dates, columns=asset_names)\n",
    "    mvo_portfolio_returns = pd.Series(index=test_dates)\n",
    "\n",
    "    # iterate through each day in the test period\n",
    "    for i, current_date in enumerate(test_dates):\n",
    "        \n",
    "        try:\n",
    "            end_loc = returns_full.index.get_loc(current_date) \n",
    "            lookback_end_loc = end_loc                \n",
    "            lookback_start_loc = lookback_end_loc - lookback_window\n",
    "\n",
    "            if lookback_start_loc < 0:\n",
    "                 print(f\"Warning: Not enough history for lookback window on {current_date}. Using equal weights.\")\n",
    "                 weights_t = np.ones(n_assets) / n_assets\n",
    "            else:\n",
    "                hist_returns = returns_full.iloc[lookback_start_loc:lookback_end_loc]\n",
    "                hist_returns = hist_returns.dropna()\n",
    "                if len(hist_returns) < n_assets + 1: \n",
    "                    print(f\"Warning: Not enough non-NaN data points ({len(hist_returns)}) in lookback for {current_date}. Using equal weights.\")\n",
    "                    weights_t = np.ones(n_assets) / n_assets\n",
    "                else:\n",
    "                    weights_t = calculate_mvo_weights(hist_returns, risk_free_rate_daily=0.0)\n",
    "\n",
    "        except KeyError:\n",
    "             print(f\"Warning: Date {current_date} not found in returns_full index. Skipping.\")\n",
    "             weights_t = np.ones(n_assets) / n_assets # Or handle differently\n",
    "\n",
    "        mvo_weights_history.loc[current_date] = weights_t\n",
    "\n",
    "        try:\n",
    "            actual_day_returns = returns_full.loc[current_date].fillna(0) \n",
    "            daily_portfolio_return = np.dot(weights_t, actual_day_returns)\n",
    "            mvo_portfolio_returns.loc[current_date] = daily_portfolio_return\n",
    "        except KeyError:\n",
    "            mvo_portfolio_returns.loc[current_date] = 0 \n",
    "\n",
    "    mvo_portfolio_returns = mvo_portfolio_returns.dropna()\n",
    "\n",
    "    return mvo_portfolio_returns, mvo_weights_history\n",
    "\n",
    "\n",
    "\n",
    "# RL Agent testing (Backtest)\n",
    "\n",
    "\n",
    "def run_rl_backtest(policy_net, actions, test_data, window_obs=60, window_state=120):\n",
    "    \"\"\"Runs the trained RL agent on the test data.\"\"\"\n",
    "    print(\"\\nRunning RL Agent backtest...\")\n",
    "    policy_net.eval() \n",
    "    env = PortfolioEnv(test_data, window_obs=window_obs, window_state=window_state)\n",
    "    start_offset = max(window_obs, window_state)\n",
    "    return_dates = test_data.index[start_offset + 1 : ]\n",
    "\n",
    "    state = env.reset()\n",
    "    rl_portfolio_returns_list = []\n",
    "    rl_weights_history_list = []\n",
    "    steps_taken = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        with torch.no_grad(): \n",
    "            obs, port_state = state\n",
    "            policy_logits, _ = policy_net(obs, port_state)\n",
    "            probs = F.softmax(policy_logits, dim=1)\n",
    "            action_idx = torch.argmax(probs, dim=1).item()\n",
    "            action_weights = actions[action_idx]\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action_weights)\n",
    "\n",
    "        rl_portfolio_returns_list.append(reward)\n",
    "        rl_weights_history_list.append(action_weights)\n",
    "        state = next_state\n",
    "        steps_taken += 1\n",
    "\n",
    "        if steps_taken > len(test_data):\n",
    "            print(\"Warning: Backtest loop exceeded data length.\")\n",
    "            break\n",
    "\n",
    "\n",
    "    if len(rl_portfolio_returns_list) == len(return_dates):\n",
    "        rl_portfolio_returns = pd.Series(rl_portfolio_returns_list, index=return_dates)\n",
    "        rl_weights_history = pd.DataFrame(rl_weights_history_list, index=return_dates, columns=test_data.columns)\n",
    "    else:\n",
    "         print(f\"Warning: Mismatch in RL return list ({len(rl_portfolio_returns_list)}) and expected dates ({len(return_dates)}). Returning partial results.\")\n",
    "         # mismatch handling\n",
    "         min_len = min(len(rl_portfolio_returns_list), len(return_dates))\n",
    "         rl_portfolio_returns = pd.Series(rl_portfolio_returns_list[:min_len], index=return_dates[:min_len])\n",
    "         rl_weights_history = pd.DataFrame(rl_weights_history_list[:min_len], index=return_dates[:min_len], columns=test_data.columns)\n",
    "\n",
    "\n",
    "    return rl_portfolio_returns, rl_weights_history\n",
    "\n",
    "\n",
    "\n",
    "# performance\n",
    "\n",
    "def calculate_performance_metrics(daily_returns, risk_free_rate_annual=0.0):\n",
    "    \"\"\"Calculates standard performance metrics from daily returns Series.\"\"\"\n",
    "    if daily_returns.empty or daily_returns.isnull().all():\n",
    "        return pd.Series({\n",
    "            'Cumulative Return': 0, 'Annualized Return': 0,\n",
    "            'Annualized Volatility': 0, 'Sharpe Ratio': np.nan, 'Max Drawdown': 0\n",
    "        }, dtype=float)\n",
    "\n",
    "    daily_returns = pd.to_numeric(daily_returns, errors='coerce').replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    cumulative_return = (1 + daily_returns).prod() - 1\n",
    "    # Calculate annualized return robustly\n",
    "    if len(daily_returns) > 0:\n",
    "         annualized_return = ((1 + daily_returns.mean()) ** 252) - 1\n",
    "    else:\n",
    "         annualized_return = 0\n",
    "\n",
    "    annualized_volatility = daily_returns.std() * np.sqrt(252)\n",
    "\n",
    "    # Sharpe Ratio (using daily returns for precision)\n",
    "    risk_free_rate_daily = (1 + risk_free_rate_annual)**(1/252) - 1\n",
    "    if annualized_volatility == 0 or pd.isna(annualized_volatility) or len(daily_returns) == 0:\n",
    "        sharpe_ratio = np.nan\n",
    "    else:\n",
    "        daily_mean_excess_ret = daily_returns.mean() - risk_free_rate_daily\n",
    "        daily_std = daily_returns.std()\n",
    "        sharpe_ratio = (daily_mean_excess_ret / daily_std) * np.sqrt(252) if daily_std > 0 else np.nan\n",
    "\n",
    "\n",
    "    # Max Drawdown\n",
    "    cum_returns_series = (1 + daily_returns).cumprod()\n",
    "    peak = cum_returns_series.expanding(min_periods=1).max()\n",
    "    drawdown = (cum_returns_series / peak) - 1\n",
    "    max_drawdown = drawdown.min()\n",
    "\n",
    "    metrics = {\n",
    "        'Cumulative Return': cumulative_return,\n",
    "        'Annualized Return': annualized_return,\n",
    "        'Annualized Volatility': annualized_volatility,\n",
    "        'Sharpe Ratio': sharpe_ratio,\n",
    "        'Max Drawdown': max_drawdown\n",
    "    }\n",
    "    return pd.Series(metrics)\n",
    "\n",
    "\n",
    "# MVO Backtest (using the full data for lookback and testing on test_data)\n",
    "mvo_returns, mvo_weights = run_mvo_backtest(merged_df, test_data, lookback_window=252)\n",
    "\n",
    "# RL Backtest\n",
    "rl_returns, rl_weights = run_rl_backtest(trained_policy, actions, test_data, window_obs=60, window_state=120)\n",
    "\n",
    "# Metrics \n",
    "\n",
    "common_index = mvo_returns.index.intersection(rl_returns.index)\n",
    "mvo_returns_aligned = mvo_returns.loc[common_index]\n",
    "rl_returns_aligned = rl_returns.loc[common_index]\n",
    "\n",
    "print(\"\\nCalculating Performance Metrics on Aligned Data...\")\n",
    "mvo_metrics = calculate_performance_metrics(mvo_returns_aligned)\n",
    "rl_metrics = calculate_performance_metrics(rl_returns_aligned)\n",
    "\n",
    "# compare results\n",
    "comparison_df = pd.DataFrame({'MVO (252d Roll)': mvo_metrics, 'RL Agent': rl_metrics})\n",
    "print(\"\\nPerformance Comparison (Test Period - Aligned Dates):\")\n",
    "print(comparison_df.applymap(lambda x: f\"{x:.4f}\" if isinstance(x, (int, float)) else x))\n",
    "\n",
    "\n",
    "#plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "(1 + mvo_returns_aligned).cumprod().plot(label='MVO Portfolio (252d Roll)')\n",
    "(1 + rl_returns_aligned).cumprod().plot(label='RL Agent Portfolio')\n",
    "plt.title(f'Portfolio Value Over Time ({common_index.min().date()} to {common_index.max().date()})')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Value (Log Scale)')\n",
    "plt.yscale('log') #\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c52742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
